# mini-transformer

Title: Mini-Transformer: A Lightweight Transformer Implementation from Scratch

Overview :
A compact, educational re-implementation of the Transformer architecture described in Attention is All You Need. The project focuses on readability, simplicity, and accessibility for learners.

Features :

decoder-only implementation with multi-head attention

includes RLHF , memory and CPU optimization Tricks

Clean, annotated code for study and modification

contains dynamic learning to handle new lexicon 

Performance :

Trained on toy language modeling datasets

Demonstrates : attention mechanisms, positional encoding, and layer normalization
