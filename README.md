# mini-transformer

Title: Mini-Transformer: A Lightweight Transformer Implementation from Scratch

Overview :
A compact, educational re-implementation of the Transformer architecture described in Attention is All You Need. The project focuses on readability, simplicity, and accessibility for learners.

Features :

Encoder-decoder implementation with multi-head attention

Trainable on small datasets with limited computational resources

Clean, annotated code for study and modification

Performance :

Trained on toy language modeling datasets

Demonstrates : attention mechanisms, positional encoding, and layer normalization
