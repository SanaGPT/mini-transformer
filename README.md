# mini-transformer
Title: Mini-Transformer: A Lightweight Transformer Implementation from Scratch  

Overview : A compact, educational re-implementation of the Transformer architecture described in Attention is All You Need. The project focuses on readability, simplicity, and accessibility for learners.  

Features :  decoder-only implementation with multi-head attention  includes RLHF , memory and CPU optimization Tricks  Clean, annotated code for study and modification  contains dynamic learning to handle new lexicon  

Performance :  Trained on toy language modeling datasets  

Demonstrates : attention mechanisms, positional encoding, and layer normalization

**aditional note : the project is incompleted yet
